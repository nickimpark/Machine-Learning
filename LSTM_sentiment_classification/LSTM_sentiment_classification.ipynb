{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.\n",
    "\n",
    "A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
    "\n",
    "LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UMICH SI650 dataset\n",
      "Forming vocabulary...\n",
      "Compiling LSTM-based model...\n",
      "Train on 854 samples, validate on 214 samples\n",
      "Epoch 1/9\n",
      "854/854 [==============================] - 7s 8ms/step - loss: 0.6559 - accuracy: 0.5972 - val_loss: 0.5983 - val_accuracy: 0.7897\n",
      "Epoch 2/9\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.4490 - accuracy: 0.8513 - val_loss: 0.3228 - val_accuracy: 0.9112\n",
      "Epoch 3/9\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2533 - accuracy: 0.9239 - val_loss: 0.1719 - val_accuracy: 0.9533\n",
      "Epoch 4/9\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.1304 - accuracy: 0.9672 - val_loss: 0.1135 - val_accuracy: 0.9766\n",
      "Epoch 5/9\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.0870 - accuracy: 0.9766 - val_loss: 0.0891 - val_accuracy: 0.9766\n",
      "Epoch 6/9\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.0677 - accuracy: 0.9813 - val_loss: 0.0924 - val_accuracy: 0.9813\n",
      "Epoch 7/9\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.0340 - accuracy: 0.9906 - val_loss: 0.1012 - val_accuracy: 0.9813\n",
      "Epoch 8/9\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.0370 - accuracy: 0.9906 - val_loss: 0.1052 - val_accuracy: 0.9813\n",
      "Epoch 9/9\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.0164 - accuracy: 0.9930 - val_loss: 0.1190 - val_accuracy: 0.9813\n",
      "214/214 [==============================] - 0s 1ms/step\n",
      "Accuracy on TEST SET:  98.13084006309509 %\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import collections\n",
    "import nltk\n",
    "\n",
    "print('Using UMICH SI650 dataset')\n",
    "\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"tweets.txt\", 'r', encoding = 'utf-8')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    num_recs += 1\n",
    "ftrain.close()\n",
    "\n",
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "\n",
    "print('Forming vocabulary...')\n",
    "\n",
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in \n",
    "                enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "X = np.empty((num_recs, ), dtype = list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(\"tweets.txt\", 'r', encoding = 'utf-8')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "X = sequence.pad_sequences(X, maxlen = MAX_SENTENCE_LENGTH)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.2, random_state = 214)\n",
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 9\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length = MAX_SENTENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout = 0.2, recurrent_dropout = 0.2, bias_regularizer=None, recurrent_activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "print('Compiling LSTM-based model...')\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(Xtrain, ytrain, batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, validation_data = (Xtest, ytest))\n",
    "\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size = BATCH_SIZE)\n",
    "print('Accuracy on TEST SET: ',(100*acc),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row - prediction, second row - target, third row - sentence. Have a fun!\n",
      "1\t1\ti liked the harry potter lines , but ron is my favourite character ...\n",
      "1\t1\tif theres one thing i love as much as harry potter , its avatar ! ! ! ..\n",
      "0\t0\ti wo n't go too far into my rant about why these people are deluded if they think harry potter is evil ( and is anything less than a loose allegory of the bible .\n",
      "1\t1\tthen again , my opinion may be a bit biased because i loved the da vinci code soundtrack . ) .\n",
      "0\t0\tits freezing cold up there ! - ... ... -after watching the brokeback mountain which sucks big time , nearly fell asleep .\n",
      "1\t1\ti miss the harry potter hookup .\n",
      "0\t0\ti 'm not even halfway through this movie , but i think brokeback mountain is terrible..\n",
      "1\t1\ti love harry potter , but every few months or so i 'll go through an intense harry potter phase .\n",
      "0\t0\ti heard da vinci code sucked pretty hard , which is too bad , because i like ron howard .\n",
      "1\t1\ti liked the harry potter lines , but ron is my favourite character ...\n"
     ]
    }
   ],
   "source": [
    "print('First row - prediction, second row - target, third row - sentence. Have a fun!')\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,40)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentim_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
